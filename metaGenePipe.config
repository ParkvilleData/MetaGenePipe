# Include the application.conf file
include required(classpath("application"))


###############################################################################
# Bobbie Shaban - Wdl call-caching config
# WDL pipeline config which uses slurm
# and creates a hypersql persistent db.
#
# This is a HOCON (Human-Optimized Config Object Notation) formated config
# file.
#
#defines slurm and creates the job submission code
# default: Defines job submission engine this instance is slurm

backend {
  default = "local"
  providers {
    Slurm {
      actor-factory = "cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"

      ## exit-code-timeout is 300 i.e. if job hangs in slurm it will kill job after 300 seconds
      config {
        exit-code-timeout-seconds = 300
        workflow-reset = true
        read_from_cache = true
        write_to_cache = true
        system.file-hash-cache=true
        concurrent-job-limit = 80
        runtime-attributes = """
          String head_directory = "$PWD"
          String singularity_image = "$PWD/metagenepipe.simg_v1.sif"
          ## change database_directory to location of blast database
          String database_directory = "/data/gpfs/datasets/BLAST/db/"
          String optim_directory = "$PWD/outputs/optimisation/"
          Int runtime_minutes = 100
          Int cpus = 2
          Int mem = 5000
          String rt_queue = "adhoc,snowy,physical"
          ## Can uncomment adhoc if running on adhoc server
          #String rt_queue = "snowy,mig"
          #String adhoc = "-q adhoc"
          String adhoc = ""
          ## If running adhoc partition change adhoc string to "-q adhoc", otherwise leave empty 
          ## please keep in mind that only punim1293 has access to adhoc partition
          String account = "--account=punim1293" 
        """
        
        submit = """
          if [ ! -d ${optim_directory} ]; then mkdir ${optim_directory}; fi;
          sbatch -J ${job_name} -D ${cwd} -o ${out} -e ${err} -t ${runtime_minutes} -p ${rt_queue} ${adhoc} ${account} ${"-n 1 -c " + cpus} --mem=${mem} \
            --wrap "module load gcc/8.3.0; module load singularity/3.5.3;
            /usr/bin/time -v --output ${optim_directory}/${job_name}.txt singularity run -B ${head_directory},${database_directory}:${head_directory},${database_directory} ${singularity_image} /bin/bash ${script}; 
            sh ${head_directory}/scripts/opt_shell.sh ${script} ${job_name} ${optim_directory}"
        """

        kill = "scancel ${job_id}"
        check-alive = "squeue -j ${job_id}"
        job-id-regex = "Submitted batch job (\\d+)"

        filesystems {
          local {
            localization: [
              "soft-link", "hard-link", "copy"
            ]

            caching {
              # When copying a cached result, what type of file duplication should occur. Attempted in the order listed below:
              duplication-strategy: [
                "hard-link", "soft-link", "copy"
              ]

              # Possible values: file, path
              # "file" will compute an md5 hash of the file content.
              # "path" will compute an md5 hash of the file path. This strategy will only be effective if the duplication-strategy (above) is set to "soft-link",
              # in order to allow for the original file path to be hashed.
              hashing-strategy: "path"

              # When true, will check if a sibling file with the same name and the .md5 extension exists, and if it does, use the content of this file as a hash.
              # If false or the md5 does not exist, will proceed with the above-defined hashing strategy.
              check-sibling-md5: false
            } #end caching
          } #end local
        } #end file system
      } #end config
    } #end slurm

    local {
      # The backend custom configuration.
      actor-factory = "cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"

      config {
        run-in-background = true
        exit-code-timeout-seconds = 300
        workflow-reset = true
        read_from_cache = true
        write_to_cache = true
        system.file-hash-cache=true
        concurrent-job-limit = 2

        runtime-attributes = """
          String head_directory = "$PWD"
          String singularity_image = "$PWD/metagenepipe.simg_v1.sif"
          String database_directory = "/data/gpfs/datasets/BLAST/db/"
          Int runtime_minutes = 100
          Int cpus = 2
          Int mem = 5000
          String? docker
          String? docker_user
        """

        submit = "if [ ! -d ./optimisation ]; then mkdir ./optimisation; fi;  module load singularity/3.5.3; /usr/bin/time -v --output ./optimisation/${job_name}.txt singularity run -B ${head_directory},${database_directory}:${head_directory},${database_directory} ${singularity_image} /bin/bash ${script}"

        filesystems {
          local {
            localization: [
              "soft-link", "hard-link", "copy"
            ]

            caching {
              # When copying a cached result, what type of file duplication should occur. Attempted in the order listed below:
              duplication-strategy: [
                "hard-link", "soft-link", "copy"
              ]

              # Possible values: file, path
              # "file" will compute an md5 hash of the file content.
              # "path" will compute an md5 hash of the file path. This strategy will only be effective if the duplication-strategy (above) is set to "soft-link",
              # in order to allow for the original file path to be hashed.
              hashing-strategy: "path"

              # When true, will check if a sibling file with the same name and the .md5 extension exists, and if it does, use the content of this file as a hash.
              # If false or the md5 does not exist, will proceed with the above-defined hashing strategy.
              check-sibling-md5: false
            } #end caching
          } #end local
        } #end file system
      } ##end config 
    } ##end sinteractive

    Local-Alternative {
      actor-factory = "cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"
      config {  

        run-in-background = true
        exit-code-timeout-seconds = 300
        workflow-reset = true
        read_from_cache = true
        write_to_cache = true
        system.file-hash-cache=true
        concurrent-job-limit = 2

        runtime-attributes = """
          String? docker
          String? docker_user
          Int? runtime_minutes
          Int? cpus
          Int? mem
          String head_directory = "$PWD"
          String singularity_image = "$PWD/metagenepipe.simg_v1.sif""
        """

        submit = "singularity run -B ${head_directory}:${head_directory} ${singularity_image} /bin/bash ${script}"

        filesystems {
          local {
            localization: [
              "soft-link", "hard-link", "copy"
            ]

            caching {
              # When copying a cached result, what type of file duplication should occur. Attempted in the order listed below:
              duplication-strategy: [
                "hard-link", "soft-link", "copy"
              ]

              # Possible values: file, path
              # "file" will compute an md5 hash of the file content.
              # "path" will compute an md5 hash of the file path. This strategy will only be effective if the duplication-strategy (above) is set to "soft-link",
              # in order to allow for the original file path to be hashed.
              hashing-strategy: "path"

              # When true, will check if a sibling file with the same name and the .md5 extension exists, and if it does, use the content of this file as a hash.
              # If false or the md5 does not exist, will proceed with the above-defined hashing strategy.
              check-sibling-md5: false
            } #end caching
          } #end local
        } #end filesystems
      } ## end config
    } ## End Local
  } #end providers
} #end back end

# Enables call caching
call-caching {
  enabled = true
  invalidate-bad-cache-results = true
}

##### Cromwell aborts jobs when a control-C command is received.
system {
  abort-jobs-on-terminate=true
}

### CRITICAL DATABASE CONFIG ###
#Explanation of the options (see also http://hsqldb.org/doc/2.0/guide/dbproperties-chapt.html):
#
#jdbc:hsqldb:file:cromwell-executions/cromwell-db/cromwell-db; This will make sure all persistence files will end up in a folder cromwell-db inside cromwell-executions.
#shutdown=false. This makes sure the database will not be shutdown unless Cromwell explicitly does so.
#hsqlldb.default_table_type=cached. By default hsqldb uses in memory tables, this will ensure data is written to disk and decrease memory usage.
#hsqldb.result_max_memory_rows=10000 . Limits the amount of rows in memory for temp tables.
#hsqldb.tx=mvcc this is a cromwell default for running with hsqldb.
#hsqldb.large_data=true. Cromwell creates huge DBs that need to be opened.
#hsqldb.applog=1. Log errors relating to the database.
#hsqldb.lob_compressed=true. Compress lobs. This saves some space. Do note that lobs are compressed individually. The total database will still contain a lot of redundancy because a lot of lobs will be similar.
#hsqldb.script_format=3. Compress script. (uses gzip internally). The script can still be opened normally after decompressing with gzip.
#connectionTimeout = 120000 opening the large database files again when running cromwell will take some time. The default timeout of 3000 ms (3s) is not enough. So it is set to 120000ms (120s).
#numThreads = 1. This will limit the CPU usage of Cromwell, which can be useful in HPC environments.
#insert-batch-size = Cromwell queues up and then inserts batches of records into the database for increased performance. You can adjust the number of database rows batch inserted by Cromwell as follows:

database {
  profile = "slick.jdbc.HsqldbProfile$"
  db {
    driver = "org.hsqldb.jdbcDriver"
    url = """
      jdbc:hsqldb:file:cromwell-executions/cromwell-db/cromwell-db;
      shutdown=false;
      hsqldb.default_table_type=cached;hsqldb.tx=mvcc;
      hsqldb.result_max_memory_rows=1000;
      hsqldb.large_data=true;
      hsqldb.applog=1;
      hsqldb.lob_compressed=true;
      hsqldb.script_format=3
    """

    connectionTimeout = 120000
    numThreads = 1
    insert-batch-size=100
  }

  migration {
    # For databases with a very large number of symbols, selecting all the rows at once can generate a variety of
    # problems. In order to avoid any issue, the selection is paginated. This value sets how many rows should be
    # retrieved and processed at a time, before asking for the next chunk.
    read-batch-size = 10000

    # Because a symbol row can contain any arbitrary wdl value, the amount of metadata rows to insert from a single
    # symbol row can vary from 1 to several thousands (or more). To keep the size of the insert batch from growing out
    # of control we monitor its size and execute/commit when it reaches or exceeds writeBatchSize.
    write-batch-size = 10000
  }
} #end databse
