# Include the application.conf file
include required(classpath("application"))


#########################################
# Bobbie Shaban - Wdl call-cahing config
# WDL pipeline config which uses slurm
# and creates a hypersql persistent db
#
#


#defines slurm and creates the job submission code
# default: Defines job submission engine this instance is slurm

backend {
default = "Slurm"
  providers {
    Slurm {
      actor-factory = "cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"
      
    ## Concurrent job limit at 32 - can be changed to more
    ## exit-cde-timeout is 300 i.e. if job hangs in slurm it will kill job after 300 seconds
    config {
        concurrent-job-limit = 32
         exit-code-timeout-seconds = 300
         workflow-reset = true
         read_from_cache = true
         write_to_cache = true
         system.file-hash-cache=true
        runtime-attributes = """
        Int runtime_minutes = 100
        Int cpus = 2
        Int mem = 5000
        String rt_queue = "snowy"
        """

        submit = """
            sbatch -J ${job_name} -D ${cwd} -o ${out} -e ${err} -t ${runtime_minutes} -p ${rt_queue} \
            ${"-n " + cpus} --mem=${mem} \
            --wrap "/bin/bash ${script}"
        """
        kill = "scancel ${job_id}"
        check-alive = "squeue -j ${job_id}"
        job-id-regex = "Submitted batch job (\\d+)"

        filesystems {
          local {
            localization: [
              "soft-link", "hard-link", "copy"
            ]

            caching {
              # When copying a cached result, what type of file duplication should occur. Attempted in the order listed below:
              duplication-strategy: [
                "soft-link", "hard-link", "copy"
              ]

              # Possible values: file, path
              # "file" will compute an md5 hash of the file content.
              # "path" will compute an md5 hash of the file path. This strategy will only be effective if the duplication-strategy (above) is set to "soft-link",
              # in order to allow for the original file path to be hashed.
              hashing-strategy: "path"

              # When true, will check if a sibling file with the same name and the .md5 extension exists, and if it does, use the content of this file as a hash.
              # If false or the md5 does not exist, will proceed with the above-defined hashing strategy.
              check-sibling-md5: false
             } #end caching
          }
        } #end file system
      } #end config
    } #end slurm
  } #end providers
} #end providers

# Enables call caching
call-caching {
         enabled = true
         invalidate-bad-cache-results = true
}

##### Cromwell aborts jobs when a control-C command is received.
system {
  abort-jobs-on-terminate=true
}

### CRITICAL DATABASE CONFIG ###
#Explanation of the options (see also http://hsqldb.org/doc/2.0/guide/dbproperties-chapt.html):
#
#jdbc:hsqldb:file:cromwell-executions/cromwell-db/cromwell-db; This will make sure all persistence files will end up in a folder cromwell-db inside cromwell-executions.
#shutdown=false. This makes sure the database will not be shutdown unless Cromwell explicitly does so.
#hsqlldb.default_table_type=cached. By default hsqldb uses in memory tables, this will ensure data is written to disk and decrease memory usage.
#hsqldb.result_max_memory_rows=10000 . Limits the amount of rows in memory for temp tables.
#hsqldb.tx=mvcc this is a cromwell default for running with hsqldb.
#hsqldb.large_data=true. Cromwell creates huge DBs that need to be opened.
#hsqldb.applog=1. Log errors relating to the database.
#hsqldb.lob_compressed=true. Compress lobs. This saves some space. Do note that lobs are compressed individually. The total database will still contain a lot of redundancy because a lot of lobs will be similar.
#hsqldb.script_format=3. Compress script. (uses gzip internally). The script can still be opened normally after decompressing with gzip.
#connectionTimeout = 120000 opening the large database files again when running cromwell will take some time. The default timeout of 3000 ms (3s) is not enough. So it is set to 120000ms (120s).
#numThreads = 1. This will limit the CPU usage of Cromwell, which can be useful in HPC environments.
#insert-batch-size = Cromwell queues up and then inserts batches of records into the database for increased performance. You can adjust the number of database rows batch inserted by Cromwell as follows:

database {
          profile = "slick.jdbc.HsqldbProfile$"
           db {
                driver = "org.hsqldb.jdbcDriver"
                url = """
                jdbc:hsqldb:file:cromwell-executions/cromwell-db/cromwell-db;
                shutdown=false;
                hsqldb.default_table_type=cached;hsqldb.tx=mvcc;
                hsqldb.result_max_memory_rows=10000;
                hsqldb.large_data=true;
                hsqldb.applog=1;
                hsqldb.lob_compressed=true;
                hsqldb.script_format=3
                """

                connectionTimeout = 120000
                numThreads = 1
                insert-batch-size=2000
        }

        migration {
                # For databases with a very large number of symbols, selecting all the rows at once can generate a variety of
                # problems. In order to avoid any issue, the selection is paginated. This value sets how many rows should be
                # retrieved and processed at a time, before asking for the next chunk.
                read-batch-size = 100000

                # Because a symbol row can contain any arbitrary wdl value, the amount of metadata rows to insert from a single
                # symbol row can vary from 1 to several thousands (or more). To keep the size of the insert batch from growing out
                # of control we monitor its size and execute/commit when it reaches or exceeds writeBatchSize.
                write-batch-size = 100000
        }
} #end databse

